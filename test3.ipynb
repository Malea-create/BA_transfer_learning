{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.10.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python310964bittransferlearningconda9726b93461e0487db588c58d1bc36ff2",
   "display_name": "Python 3.10.9 64-bit ('transfer-learning': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as al\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "def one_hot(y, fill_k=False, one_not=False):\n",
    "    \"\"\"Map to one-hot encoding.\"\"\"\n",
    "    # Check labels\n",
    "    labels = np.unique(y)\n",
    "\n",
    "    # Number of classes\n",
    "    K = len(labels)\n",
    "\n",
    "    # Number of samples\n",
    "    N = y.shape[0]\n",
    "\n",
    "    # Preallocate array\n",
    "    if one_not:\n",
    "        Y = -np.ones((N, K))\n",
    "    else:\n",
    "        Y = np.zeros((N, K))\n",
    "\n",
    "    # Set k-th column to 1 for n-th sample\n",
    "    for n in range(N):\n",
    "\n",
    "        # Map current class to index label\n",
    "        y_n = (y[n] == labels)\n",
    "\n",
    "        if fill_k:\n",
    "            Y[n, y_n] = y_n\n",
    "        else:\n",
    "            Y[n, y_n] = 1\n",
    "\n",
    "    return Y, labels\n",
    "\n",
    "\n",
    "def regularize_matrix(A, a=0.0):\n",
    "    \"\"\"\n",
    "    Regularize matrix by ensuring minimum eigenvalues.\n",
    "    INPUT   (1) array 'A': square matrix\n",
    "            (2) float 'a': constraint on minimum eigenvalue\n",
    "    OUTPUT  (1) array 'B': constrained matrix\n",
    "    \"\"\"\n",
    "    # Check for square matrix\n",
    "    N, M = A.shape\n",
    "    if not N == M:\n",
    "        raise ValueError('Matrix not square.')\n",
    "\n",
    "    # Check for valid matrix entries\n",
    "    if np.any(np.isnan(A)) or np.any(np.isinf(A)):\n",
    "        raise ValueError('Matrix contains NaNs or infinities.')\n",
    "\n",
    "    # Check for non-negative minimum eigenvalue\n",
    "    if a < 0:\n",
    "        raise ValueError('minimum eigenvalue cannot be negative.')\n",
    "\n",
    "    elif a == 0:\n",
    "        return A\n",
    "\n",
    "    else:\n",
    "        # Ensure symmetric matrix\n",
    "        A = (A + A.T) / 2\n",
    "\n",
    "        # Eigenvalue decomposition\n",
    "        E, V = al.eig(A)\n",
    "\n",
    "        # Regularization matrix\n",
    "        aI = a * np.eye(N)\n",
    "\n",
    "        # Subtract regularization\n",
    "        E = np.diag(E) + aI\n",
    "\n",
    "        # Cap negative eigenvalues at zero\n",
    "        E = np.maximum(0, E)\n",
    "\n",
    "        # Reconstruct matrix\n",
    "        B = np.dot(np.dot(V, E), V.T)\n",
    "\n",
    "        # Add back subtracted regularization\n",
    "        return B + aI\n",
    "\n",
    "\n",
    "def is_pos_def(X):\n",
    "    \"\"\"Check for positive definiteness.\"\"\"\n",
    "    return np.all(np.linalg.eigvals(X) > 0)\n",
    "\n",
    "\n",
    "def nullspace(A, atol=1e-13, rtol=0):\n",
    "    \"\"\"\n",
    "    Compute an approximate basis for the nullspace of A.\n",
    "    INPUT   (1) array 'A': 1-D array with length k will be treated\n",
    "                as a 2-D with shape (1, k).\n",
    "            (2) float 'atol': the absolute tolerance for a zero singular value.\n",
    "                Singular values smaller than `atol` are considered to be zero.\n",
    "            (3) float 'rtol': relative tolerance. Singular values less than\n",
    "                rtol*smax are considered to be zero, where smax is the largest\n",
    "                singular value.\n",
    "                If both `atol` and `rtol` are positive, the combined tolerance\n",
    "                is the maximum of the two; tol = max(atol, rtol * smax)\n",
    "                Singular values smaller than `tol` are considered to be zero.\n",
    "    OUTPUT  (1) array 'B': if A is an array with shape (m, k), then B will be\n",
    "                an array with shape (k, n), where n is the estimated dimension\n",
    "                of the nullspace of A.  The columns of B are a basis for the\n",
    "                nullspace; each element in np.dot(A, B) will be\n",
    "                approximately zero.\n",
    "    \"\"\"\n",
    "    # Expand A to a matrix\n",
    "    A = np.atleast_2d(A)\n",
    "\n",
    "    # Singular value decomposition\n",
    "    u, s, vh = al.svd(A)\n",
    "\n",
    "    # Set tolerance\n",
    "    tol = max(atol, rtol * s[0])\n",
    "\n",
    "    # Compute the number of non-zero entries\n",
    "    nnz = (s >= tol).sum()\n",
    "\n",
    "    # Conjugate and transpose to ensure real numbers\n",
    "    ns = vh[nnz:].conj().T\n",
    "\n",
    "    return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from scipy.sparse.linalg import eigs\n",
    "from scipy.spatial.distance import cdist\n",
    "import sklearn as sk\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from os.path import basename\n",
    "\n",
    "\n",
    "class TransferComponentClassifier(object):\n",
    "    \"\"\"\n",
    "    Class of classifiers based on Transfer Component Analysis.\n",
    "    Methods contain component analysis and general utilities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 loss_function='logistic',\n",
    "                 l2_regularization=1.0,\n",
    "                 mu=1.0,\n",
    "                 num_components=1,\n",
    "                 kernel_type='rbf',\n",
    "                 bandwidth=1.0,\n",
    "                 order=2.0):\n",
    "        \"\"\"\n",
    "        Select a particular type of transfer component classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss_function : str\n",
    "            loss function for weighted classifier, options: 'logistic',\n",
    "            'quadratic', 'hinge' (def: 'logistic')\n",
    "        l2 : float\n",
    "            l2-regularization parameter value (def:0.01)\n",
    "        mu : float\n",
    "            trade-off parameter (def: 1.0)\n",
    "        num_components : int\n",
    "            number of transfer components to maintain (def: 1)\n",
    "        kernel_type : str\n",
    "            type of kernel to use, options: 'rbf' (def: 'rbf')\n",
    "        bandwidth : float\n",
    "            kernel bandwidth for transfer component analysis (def: 1.0)\n",
    "        order : float\n",
    "            order of polynomial for kernel (def: 2.0)\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        Attributes\n",
    "        ----------\n",
    "        loss\n",
    "            which loss function to use\n",
    "        is_trained\n",
    "            whether the classifier has been trained on data already\n",
    "        \"\"\"\n",
    "        self.loss = loss_function\n",
    "        self.l2 = l2_regularization\n",
    "        self.mu = mu\n",
    "        self.num_components = num_components\n",
    "\n",
    "        self.kernel_type = kernel_type\n",
    "        self.bandwidth = bandwidth\n",
    "        self.order = order\n",
    "\n",
    "        # Initialize untrained classifiers\n",
    "        if self.loss in ('lr', 'logr', 'logistic'):\n",
    "\n",
    "            if l2_regularization:\n",
    "\n",
    "                # Logistic regression model with fixed regularization\n",
    "                self.clf = LinearRegression()\n",
    "\n",
    "            else:\n",
    "                # Logistic regression model, cross-validated for regularization\n",
    "                self.clf = LinearRegressionCV()\n",
    "\n",
    "        elif self.loss in ('squared', 'qd', 'quadratic'):\n",
    "\n",
    "            if l2_regularization:\n",
    "\n",
    "                # Least-squares model with fixed regularization\n",
    "                self.clf = Ridge()\n",
    "\n",
    "            else:\n",
    "                # Least-squares model, cross-validated for regularization\n",
    "                self.clf = Ridge()\n",
    "\n",
    "        elif self.loss in ('hinge', 'linsvm', 'linsvr'):\n",
    "\n",
    "            # Linear support vector machine\n",
    "            self.clf = LinearSVR()\n",
    "\n",
    "        elif self.loss in ('rbfsvr', 'rbfsvm'):\n",
    "\n",
    "            # Radial basis function support vector machine\n",
    "            self.clf = SVR()\n",
    "        else:\n",
    "            # Other loss functions are not implemented\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Maintain source and transfer data for computing kernels\n",
    "        self.XZ = ''\n",
    "\n",
    "        # Maintain transfer components\n",
    "        self.C = ''\n",
    "\n",
    "        # Whether model has been trained\n",
    "        self.is_trained = False\n",
    "\n",
    "        # Dimensionality of training data\n",
    "        self.train_data_dim = ''\n",
    "\n",
    "    def kernel(self, X, Z, type='rbf', order=2, bandwidth=1.0):\n",
    "        \"\"\"\n",
    "        Compute kernel for given data set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array\n",
    "            data set (N samples by D features)\n",
    "        Z : array\n",
    "            data set (M samples by D features)\n",
    "        type : str\n",
    "            type of kernel, options: 'linear', 'polynomial', 'rbf',\n",
    "            'sigmoid' (def: 'linear')\n",
    "        order : float\n",
    "            degree for the polynomial kernel (def: 2.0)\n",
    "        bandwidth : float\n",
    "            kernel bandwidth (def: 1.0)\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            kernel matrix (N+M by N+M)\n",
    "        \"\"\"\n",
    "        # Data shapes\n",
    "        N, DX = X.shape\n",
    "        M, DZ = Z.shape\n",
    "\n",
    "        # Assert equivalent dimensionalities\n",
    "        if not DX == DZ:\n",
    "            raise ValueError('Dimensionalities of X and Z should be equal.')\n",
    "\n",
    "        # Select type of kernel to compute\n",
    "        if type == 'linear':\n",
    "            # Linear kernel is data outer product\n",
    "            return np.dot(X, Z.T)\n",
    "        elif type == 'polynomial':\n",
    "            # Polynomial kernel is an exponentiated data outer product\n",
    "            return (np.dot(X, Z.T) + 1)**p\n",
    "        elif type == 'rbf':\n",
    "            # Radial basis function kernel\n",
    "            return np.exp(-cdist(X, Z) / (2.*bandwidth**2))\n",
    "        elif type == 'sigmoid':\n",
    "            # Sigmoidal kernel\n",
    "            return 1./(1 + np.exp(np.dot(X, Z.T)))\n",
    "        else:\n",
    "            raise NotImplementedError('Loss not implemented yet.')\n",
    "\n",
    "    def transfer_component_analysis(self, X, Z):\n",
    "        \"\"\"\n",
    "        Transfer Component Analysis.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array\n",
    "            source data set (N samples by D features)\n",
    "        Z : array\n",
    "            target data set (M samples by D features)\n",
    "        Returns\n",
    "        -------\n",
    "        C : array\n",
    "            transfer components (D features by num_components)\n",
    "        K : array\n",
    "            source and target data kernel distances\n",
    "        \"\"\"\n",
    "        # Data shapes\n",
    "        N, DX = X.shape\n",
    "        M, DZ = Z.shape\n",
    "\n",
    "        # Assert equivalent dimensionalities\n",
    "        if not DX == DZ:\n",
    "            raise ValueError('Dimensionalities of X and Z should be equal.')\n",
    "\n",
    "        # Compute kernel matrix\n",
    "        XZ = np.concatenate((X, Z), axis=0)\n",
    "        K = self.kernel(XZ, XZ, type=self.kernel_type,\n",
    "                        bandwidth=self.bandwidth)\n",
    "\n",
    "        # Ensure positive-definiteness\n",
    "        if not is_pos_def(K):\n",
    "            print('Warning: covariate matrices not PSD.')\n",
    "\n",
    "            regct = -6\n",
    "            while not is_pos_def(K):\n",
    "                print('Adding regularization: ' + str(10**regct))\n",
    "\n",
    "                # Add regularization\n",
    "                K += np.eye(N + M)*10.**regct\n",
    "\n",
    "                # Increment regularization counter\n",
    "                regct += 1\n",
    "\n",
    "        # Normalization matrix\n",
    "        L = np.vstack((np.hstack((np.ones((N, N))/N**2,\n",
    "                                  -1*np.ones((N, M))/(N*M))),\n",
    "                       np.hstack((-1*np.ones((M, N))/(N*M),\n",
    "                                  np.ones((M, M))/M**2))))\n",
    "\n",
    "        # Centering matrix\n",
    "        H = np.eye(N + M) - np.ones((N + M, N + M)) / float(N + M)\n",
    "\n",
    "        # Matrix Lagrangian objective function: (I + mu*K*L*K)^{-1}*K*H*K\n",
    "        J = np.dot(np.linalg.inv(np.eye(N + M) +\n",
    "                   self.mu*np.dot(np.dot(K, L), K)),\n",
    "                   np.dot(np.dot(K, H), K))\n",
    "\n",
    "        # Eigenvector decomposition as solution to trace minimization\n",
    "        _, C = eigs(J, k=self.num_components)\n",
    "\n",
    "        # Discard imaginary numbers (possible computation issue)\n",
    "        return np.real(C), K\n",
    "\n",
    "    def fit(self, X, y, Z):\n",
    "        \"\"\"\n",
    "        Fit/train a classifier on data mapped onto transfer components.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array\n",
    "            source data (N samples by D features)\n",
    "        y : array\n",
    "            source labels (N samples by 1)\n",
    "        Z : array\n",
    "            target data (M samples by D features)\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Data shapes\n",
    "        N, DX = X.shape\n",
    "        M, DZ = Z.shape\n",
    "\n",
    "        # Assert equivalent dimensionalities\n",
    "        if not DX == DZ:\n",
    "            raise ValueError('Dimensionalities of X and Z should be equal.')\n",
    "\n",
    "        # Assert correct number of components for given dataset\n",
    "        if not self.num_components <= N + M - 1:\n",
    "            raise ValueError('''Number of components must be smaller than or\n",
    "                             equal to the source sample size plus target sample\n",
    "                             size plus 1.''')\n",
    "\n",
    "        # Maintain source and target data for later kernel computations\n",
    "        self.XZ = np.concatenate((X, Z), axis=0)\n",
    "\n",
    "        # Transfer component analysis\n",
    "        self.C, K = self.transfer_component_analysis(X, Z)\n",
    "\n",
    "        # Map source data onto transfer components\n",
    "        X = np.dot(K[:N, :], self.C)\n",
    "\n",
    "        # Train a weighted classifier\n",
    "        if self.loss in ('lr', 'logr', 'logistic'):\n",
    "\n",
    "            # Logistic regression model with sample weights\n",
    "            self.clf.fit(X, y)\n",
    "\n",
    "        elif self.loss in ('squared', 'qd', 'quadratic'):\n",
    "\n",
    "            # Least-squares model with sample weights\n",
    "            self.clf.fit(X, y)\n",
    "\n",
    "        elif self.loss in ('hinge', 'linsvm', 'linsvr'):\n",
    "\n",
    "            # Linear support vector machine with sample weights\n",
    "            self.clf.fit(X, y)\n",
    "        else:\n",
    "            # Other loss functions are not implemented\n",
    "            raise NotImplementedError('Loss not implemented yet.')\n",
    "\n",
    "        # Mark classifier as trained\n",
    "        self.is_trained = True\n",
    "\n",
    "        # Store training data dimensionality\n",
    "        self.train_data_dim = DX\n",
    "\n",
    "    def predict(self, Z):\n",
    "        \"\"\"\n",
    "        Make predictions on new dataset.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : array\n",
    "            new data set (M samples by D features)\n",
    "        Returns\n",
    "        -------\n",
    "        preds : array\n",
    "            label predictions (M samples by 1)\n",
    "        \"\"\"\n",
    "        # Data shape\n",
    "        M, D = Z.shape\n",
    "\n",
    "        # If classifier is trained, check for same dimensionality\n",
    "        if self.is_trained:\n",
    "            if not self.train_data_dim == D:\n",
    "                raise ValueError('''Test data is of different dimensionality\n",
    "                                 than training data.''')\n",
    "\n",
    "        # Compute kernel for new data\n",
    "        K = self.kernel(Z, self.XZ, type=self.kernel_type,\n",
    "                        bandwidth=self.bandwidth, order=self.order)\n",
    "\n",
    "        # Map new data onto transfer components\n",
    "        Z = np.dot(K, self.C)\n",
    "\n",
    "        # Call scikit's predict function\n",
    "        preds = self.clf.predict(Z)\n",
    "\n",
    "        # For quadratic loss function, correct predictions\n",
    "        if self.loss == 'quadratic':\n",
    "            preds = (np.sign(preds)+1)/2.\n",
    "\n",
    "        # Return predictions array\n",
    "        return preds\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"Get classifier parameters.\"\"\"\n",
    "        return self.clf.get_params()\n",
    "\n",
    "    def is_trained(self):\n",
    "        \"\"\"Check whether classifier is trained.\"\"\"\n",
    "        return self.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Column: 1.00 has the highest correlation with the target: 0.49783558206163886\nDf has been split into 3 equal parts:  (334, 9) (334, 9) (334, 9)\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split according to distribution\n",
    "\n",
    "df = pd.read_csv(\"data\\ICS\\Concrete_Data_2.csv\", \";\")\n",
    "df= df.fillna(0)\n",
    "\n",
    "# Calculate Correlation between coloumns \n",
    "corr_values = []\n",
    "highest_corr = 0\n",
    "highest_col = 0\n",
    "\n",
    "for col in df:\n",
    "    corr = df[\"9.00\"].corr(df[col])\n",
    "    corr_values.append(corr)\n",
    "    if corr >= max(corr_values) and corr < 1.0:\n",
    "        highest_corr = corr\n",
    "        highest_col = col\n",
    "    #print(\"Correlation between the target and \"+ str(col) + \" : \" + str(corr))\n",
    "    #print(highest_corr)\n",
    "\n",
    "# Selecto Corr >= 0,4 and sort data accordingly \n",
    "print (\"Column: \"+str(highest_col)+ \" has the highest correlation with the target: \" + str(highest_corr))\n",
    "df = df.sort_values(by=[highest_col])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Split df in 3 equal parts\n",
    "split = int(len(df[:1000])/3)\n",
    "df_1 = df.loc[0:split,:]\n",
    "df_2 = df.loc[split:split*2,:]\n",
    "df_3 = df.loc[split*2:split*3,:]\n",
    "print(\"Df has been split into 3 equal parts: \",df_1.shape,df_2.shape,df_3.shape)\n",
    "\n",
    "df_src = df_2 #.append(df_2)\n",
    "#print(df_src.head(), df_src.shape)\n",
    "\n",
    "df_tar = df_3\n",
    "#print(df_tar.head(), df_tar.shape)\n",
    "\n",
    "# source\n",
    "Xs = df_src.iloc[:,:-1]\n",
    "Ys = df_src.iloc[:,-1]\n",
    "\n",
    "# target_train\n",
    "Xt = df_tar.iloc[:,:-1]\n",
    "Yt = df_tar.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TransferComponentClassifier()\n",
    "assert type(clf) == TransferComponentClassifier\n",
    "assert not clf.is_trained\n",
    "\n",
    "X = rnd.randn(10, 2)\n",
    "y = np.hstack((-np.ones((5,)), np.ones((5,))))\n",
    "Z = rnd.randn(10, 2) + 1\n",
    "clf = TransferComponentClassifier()\n",
    "clf.fit(X, y, Z)\n",
    "assert clf.is_trained\n",
    "\n",
    "X = rnd.randn(10, 2)\n",
    "y = np.hstack((-np.ones((5,)), np.ones((5,))))\n",
    "Z = rnd.randn(10, 2) + 1\n",
    "clf = TransferComponentClassifier()\n",
    "clf.fit(X, y, Z)\n",
    "u_pred = clf.predict(Z)\n",
    "labels = np.unique(y)\n",
    "assert len(np.setdiff1d(np.unique(u_pred), labels)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TransferComponentClassifier()\n",
    "assert type(clf) == TransferComponentClassifier\n",
    "assert not clf.is_trained\n",
    "\n",
    "clf.fit(Xs, Ys, Xt[:25])\n",
    "u_pred = clf.predict(Xt[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "666    66.60\n667    36.80\n668    40.93\n669    28.80\n670    38.46\n671    46.23\n672    46.23\n673    38.46\n674    81.75\n675    38.70\n676    24.44\n677    40.06\n678    40.06\n679    68.10\n680    33.40\n681    25.20\n682    66.10\n683    55.50\n684    37.26\n685    57.21\n686    57.22\n687    37.27\n688    52.42\n689    31.18\n690    29.59\nName: 9.00, dtype: float64"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yt[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([33.97998583, 33.91628892, 33.91869779, 33.87146743, 33.90598948,\n       33.91227457, 33.91227457, 33.90598948, 33.86604595, 33.86604657,\n       33.86604657, 33.89402225, 33.89402226, 33.86604595, 33.86950762,\n       33.86950715, 33.86604597, 33.86604677, 33.89520816, 33.89953753,\n       33.89953753, 33.89520816, 33.86604595, 33.86604595, 33.86604651])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}